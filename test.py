import os
import json
import pickle
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import streamlit as st
from dotenv import load_dotenv
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.docstore.document import Document
import io
from docx import Document as DocxDocument
import PyPDF2
import pandas as pd
import hashlib
import time

# Load environment variables
load_dotenv()

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
CREDENTIALS_FILE = 'credentials.json'
TOKEN_FILE = 'token.pickle'
VECTOR_STORE_DIR = 'vector_store'
CACHE_DIR = 'cache'

class GoogleDriveRAGChatbot:
    def __init__(self, gemini_api_key: Optional[str] = None):
        """
        Kh·ªüi t·∫°o chatbot RAG v·ªõi c·∫•u h√¨nh n√¢ng cao
        """
        # L·∫•y API key t·ª´ env ho·∫∑c parameter
        self.gemini_api_key = gemini_api_key or os.getenv('GEMINI_API_KEY')
        
        if not self.gemini_api_key:
            raise ValueError("C·∫ßn c√≥ Gemini API Key")
        
        # C·∫•u h√¨nh Gemini
        genai.configure(api_key=self.gemini_api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash-8b')
        
        # Kh·ªüi t·∫°o embeddings
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=self.gemini_api_key
        )
        
        # C·∫•u h√¨nh t·ª´ environment
        self.chunk_size = int(os.getenv('CHUNK_SIZE', 1000))
        self.chunk_overlap = int(os.getenv('CHUNK_OVERLAP', 200))
        self.max_search_results = int(os.getenv('MAX_SEARCH_RESULTS', 5))
        self.max_file_size_mb = int(os.getenv('MAX_FILE_SIZE_MB', 50))
        
        # Text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
        )
        
        # Vector store v√† cache
        self.vector_store = None
        self.drive_service = None
        self.file_cache = {}
        
        # T·∫°o th∆∞ m·ª•c cache n·∫øu ch∆∞a c√≥
        os.makedirs(CACHE_DIR, exist_ok=True)
        
        # Load cache n·∫øu c√≥
        self._load_file_cache()
    
    def _load_file_cache(self):
        """Load cache c·ªßa files ƒë√£ x·ª≠ l√Ω"""
        cache_file = os.path.join(CACHE_DIR, 'file_cache.json')
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.file_cache = json.load(f)
                logger.info(f"Loaded cache for {len(self.file_cache)} files")
            except Exception as e:
                logger.error(f"Error loading cache: {e}")
                self.file_cache = {}
    
    def _save_file_cache(self):
        """L∆∞u cache c·ªßa files"""
        cache_file = os.path.join(CACHE_DIR, 'file_cache.json')
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_cache, f, ensure_ascii=False, indent=2)
            logger.info("File cache saved")
        except Exception as e:
            logger.error(f"Error saving cache: {e}")
    
    def _get_file_hash(self, file_id: str, modified_time: str) -> str:
        """T·∫°o hash ƒë·ªÉ check file ƒë√£ thay ƒë·ªïi ch∆∞a"""
        return hashlib.md5(f"{file_id}_{modified_time}".encode()).hexdigest()
    
    def authenticate_google_drive(self) -> bool:
        """X√°c th·ª±c Google Drive API v·ªõi error handling t·ªët h∆°n"""
        try:
            creds = None
            
            # Ki·ªÉm tra token ƒë√£ l∆∞u
            if os.path.exists(TOKEN_FILE):
                with open(TOKEN_FILE, 'rb') as token:
                    creds = pickle.load(token)
            
            # Refresh token n·∫øu c·∫ßn
            if not creds or not creds.valid:
                if creds and creds.expired and creds.refresh_token:
                    creds.refresh(Request())
                else:
                    if not os.path.exists(CREDENTIALS_FILE):
                        st.error("‚ùå Kh√¥ng t√¨m th·∫•y file credentials.json")
                        st.info("üìã H∆∞·ªõng d·∫´n t·∫°o credentials.json:")
                        st.markdown("""
                        1. Truy c·∫≠p [Google Cloud Console](https://console.cloud.google.com/)
                        2. T·∫°o project v√† enable Google Drive API
                        3. T·∫°o OAuth 2.0 credentials (Desktop application)
                        4. T·∫£i file JSON v√† ƒë·ªïi t√™n th√†nh credentials.json
                        """)
                        return False
                    
                    flow = InstalledAppFlow.from_client_secrets_file(
                        CREDENTIALS_FILE, SCOPES)
                    creds = flow.run_local_server(port=0)
                
                # L∆∞u credentials
                with open(TOKEN_FILE, 'wb') as token:
                    pickle.dump(creds, token)
            
            # T·∫°o service
            self.drive_service = build('drive', 'v3', credentials=creds)
            
            # Test connection
            self.drive_service.about().get(fields="user").execute()
            logger.info("Google Drive authentication successful")
            return True
            
        except Exception as error:
            logger.error(f"Authentication error: {error}")
            st.error(f"‚ùå L·ªói x√°c th·ª±c: {error}")
            return False
    
    def get_files_from_drive(self, 
                           folder_id: Optional[str] = None, 
                           file_types: Optional[List[str]] = None,
                           max_files: int = 100) -> List[Dict]:
        """
        L·∫•y danh s√°ch files t·ª´ Google Drive v·ªõi filtering n√¢ng cao
        """
        if not self.drive_service:
            return []
        
        try:
            # X√¢y d·ª±ng query
            query_parts = ["trashed=false"]
            
            if folder_id:
                query_parts.append(f"parents in '{folder_id}'")
            
            # Filter theo file types
            if file_types:
                mime_queries = []
                mime_map = {
                    'pdf': 'application/pdf',
                    'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    'txt': 'text/plain',
                    'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                }
                
                for file_type in file_types:
                    if file_type in mime_map:
                        mime_queries.append(f"mimeType='{mime_map[file_type]}'")
                
                if mime_queries:
                    query_parts.append(f"({' or '.join(mime_queries)})")
            
            # Filter theo k√≠ch th∆∞·ªõc (Google Drive API kh√¥ng h·ªó tr·ª£ tr·ª±c ti·∫øp)
            query = " and ".join(query_parts)
            
            # L·∫•y files v·ªõi pagination
            all_files = []
            page_token = None
            
            while len(all_files) < max_files:
                results = self.drive_service.files().list(
                    q=query,
                    pageSize=min(100, max_files - len(all_files)),
                    pageToken=page_token,
                    fields="nextPageToken, files(id, name, mimeType, size, modifiedTime, webViewLink)"
                ).execute()
                
                files = results.get('files', [])
                
                # Filter theo k√≠ch th∆∞·ªõc
                filtered_files = []
                for file_info in files:
                    file_size = int(file_info.get('size', 0))
                    if file_size <= self.max_file_size_mb * 1024 * 1024:
                        filtered_files.append(file_info)
                    else:
                        logger.warning(f"File {file_info['name']} qu√° l·ªõn ({file_size/1024/1024:.1f}MB)")
                
                all_files.extend(filtered_files)
                
                page_token = results.get('nextPageToken')
                if not page_token:
                    break
            
            logger.info(f"Found {len(all_files)} eligible files")
            return all_files[:max_files]
            
        except HttpError as error:
            logger.error(f"Error getting files: {error}")
            st.error(f"‚ùå L·ªói khi l·∫•y danh s√°ch files: {error}")
            return []
    
    def download_file_content(self, file_id: str, mime_type: str, file_name: str) -> str:
        """
        T·∫£i n·ªôi dung file v·ªõi caching v√† error handling
        """
        try:
            # Ki·ªÉm tra cache tr∆∞·ªõc
            file_info = self.drive_service.files().get(
                fileId=file_id, 
                fields="modifiedTime"
            ).execute()
            
            file_hash = self._get_file_hash(file_id, file_info['modifiedTime'])
            
            # Ki·ªÉm tra cache
            if file_hash in self.file_cache:
                logger.info(f"Using cached content for {file_name}")
                return self.file_cache[file_hash]
            
            # T·∫£i file m·ªõi
            logger.info(f"Downloading {file_name}")
            request = self.drive_service.files().get_media(fileId=file_id)
            file_content = io.BytesIO()
            
            import googleapiclient.http
            downloader = googleapiclient.http.MediaIoBaseDownload(file_content, request)
            done = False
            while done is False:
                status, done = downloader.next_chunk()
                if status:
                    progress = int(status.progress() * 100)
                    if progress % 20 == 0:  # Log m·ªói 20%
                        logger.info(f"Download progress: {progress}%")
            
            file_content.seek(0)
            
            # X·ª≠ l√Ω theo lo·∫°i file
            content = ""
            if mime_type == 'application/pdf':
                content = self._extract_pdf_text(file_content)
            elif mime_type == 'application/vnd.openxmlformats-officedocument.wordprocessingml.document':
                content = self._extract_docx_text(file_content)
            elif mime_type == 'text/plain':
                content = file_content.read().decode('utf-8', errors='ignore')
            elif mime_type == 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet':
                content = self._extract_xlsx_text(file_content)
            else:
                logger.warning(f"Unsupported file type: {mime_type}")
                return "Lo·∫°i file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£"
            
            # L∆∞u v√†o cache
            if content:
                self.file_cache[file_hash] = content
                self._save_file_cache()
            
            return content
                
        except Exception as error:
            logger.error(f"Error downloading file {file_name}: {error}")
            return f"L·ªói khi t·∫£i file: {str(error)}"
    
    def _extract_pdf_text(self, file_content: io.BytesIO) -> str:
        """Tr√≠ch xu·∫•t text t·ª´ PDF v·ªõi error handling"""
        try:
            pdf_reader = PyPDF2.PdfReader(file_content)
            text = ""
            for i, page in enumerate(pdf_reader.pages):
                try:
                    text += page.extract_text() + "\n"
                except Exception as e:
                    logger.warning(f"Error extracting page {i}: {e}")
                    continue
            return text.strip()
        except Exception as e:
            logger.error(f"PDF extraction error: {e}")
            return f"L·ªói khi ƒë·ªçc PDF: {str(e)}"
    
    def _extract_docx_text(self, file_content: io.BytesIO) -> str:
        """Tr√≠ch xu·∫•t text t·ª´ DOCX"""
        try:
            doc = DocxDocument(file_content)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            # X·ª≠ l√Ω tables n·∫øu c√≥
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        text += cell.text + "\t"
                    text += "\n"
            
            return text.strip()
        except Exception as e:
            logger.error(f"DOCX extraction error: {e}")
            return f"L·ªói khi ƒë·ªçc DOCX: {str(e)}"
    
    def _extract_xlsx_text(self, file_content: io.BytesIO) -> str:
        """Tr√≠ch xu·∫•t text t·ª´ XLSX"""
        try:
            df = pd.read_excel(file_content, sheet_name=None)  # ƒê·ªçc t·∫•t c·∫£ sheets
            text = ""
            
            for sheet_name, sheet_data in df.items():
                text += f"\n=== Sheet: {sheet_name} ===\n"
                text += sheet_data.to_string(index=False) + "\n"
            
            return text.strip()
        except Exception as e:
            logger.error(f"XLSX extraction error: {e}")
            return f"L·ªói khi ƒë·ªçc XLSX: {str(e)}"
    
    def process_documents(self, files_info: List[Dict]) -> List[Document]:
        """
        X·ª≠ l√Ω documents v·ªõi progress tracking
        """
        documents = []
        
        # Progress bar
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, file_info in enumerate(files_info):
            # Update progress
            progress = (i + 1) / len(files_info)
            progress_bar.progress(progress)
            status_text.text(f"ƒêang x·ª≠ l√Ω: {file_info['name']} ({i+1}/{len(files_info)})")
            
            # T·∫£i n·ªôi dung file
            content = self.download_file_content(
                file_info['id'], 
                file_info['mimeType'], 
                file_info['name']
            )
            
            if content and len(content.strip()) > 50:  # Ch·ªâ x·ª≠ l√Ω file c√≥ n·ªôi dung
                # T·∫°o Document object
                doc = Document(
                    page_content=content,
                    metadata={
                        'source': file_info['name'],
                        'file_id': file_info['id'],
                        'mime_type': file_info['mimeType'],
                        'size': file_info.get('size', 0),
                        'modified_time': file_info.get('modifiedTime', ''),
                        'web_view_link': file_info.get('webViewLink', ''),
                        'content_length': len(content)
                    }
                )
                documents.append(doc)
            else:
                logger.warning(f"File {file_info['name']} has no extractable content")
        
        # Clear progress
        progress_bar.empty()
        status_text.empty()
        
        logger.info(f"Processed {len(documents)} documents successfully")
        return documents
    
    def create_vector_store(self, documents: List[Document]):
        """
        T·∫°o vector store v·ªõi caching
        """
        if not documents:
            st.warning("‚ö†Ô∏è Kh√¥ng c√≥ documents ƒë·ªÉ x·ª≠ l√Ω")
            return
        
        # Ki·ªÉm tra xem ƒë√£ c√≥ vector store cache ch∆∞a
        if os.path.exists(VECTOR_STORE_DIR):
            try:
                self.vector_store = FAISS.load_local(VECTOR_STORE_DIR, self.embeddings)
                st.info("üìö ƒê√£ t·∫£i vector store t·ª´ cache")
                return
            except Exception as e:
                logger.warning(f"Could not load cached vector store: {e}")
        
        # Chia nh·ªè documents
        with st.spinner("üîß ƒêang chia nh·ªè documents..."):
            texts = self.text_splitter.split_documents(documents)
        
        st.info(f"üìù ƒê√£ t·∫°o {len(texts)} ƒëo·∫°n text t·ª´ {len(documents)} documents")
        
        # T·∫°o embeddings v·ªõi progress
        with st.spinner("ü§ñ ƒêang t·∫°o embeddings..."):
            # Batch processing ƒë·ªÉ tr√°nh rate limit
            batch_size = 10
            all_texts = []
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i+batch_size]
                all_texts.extend(batch)
                
                if i > 0:  # Delay ƒë·ªÉ tr√°nh rate limit
                    time.sleep(1)
        
        # T·∫°o vector store
        self.vector_store = FAISS.from_documents(all_texts, self.embeddings)
        
        # L∆∞u cache
        try:
            self.vector_store.save_local(VECTOR_STORE_DIR)
            logger.info("Vector store saved to cache")
        except Exception as e:
            logger.warning(f"Could not save vector store cache: {e}")
        
        st.success(f"‚úÖ ƒê√£ t·∫°o vector store v·ªõi {len(all_texts)} ƒëo·∫°n text")
    
    def search_similar_documents(self, query: str, k: Optional[int] = None) -> List[Document]:
        """
        T√¨m ki·∫øm documents t∆∞∆°ng t·ª± v·ªõi scoring
        """
        if not self.vector_store:
            return []
        
        k = k or self.max_search_results
        
        try:
            # T√¨m ki·∫øm v·ªõi score
            similar_docs_with_scores = self.vector_store.similarity_search_with_score(query, k=k)
            
            # Filter theo threshold score (t√πy ch·ªçn)
            score_threshold = 0.8
            filtered_docs = [
                doc for doc, score in similar_docs_with_scores 
                if score < score_threshold  # FAISS tr·∫£ v·ªÅ distance (c√†ng nh·ªè c√†ng t·ªët)
            ]
            
            if not filtered_docs:
                # N·∫øu kh√¥ng c√≥ doc n√†o ƒë·∫°t threshold, l·∫•y top results
                filtered_docs = [doc for doc, _ in similar_docs_with_scores[:k//2]]
            
            logger.info(f"Found {len(filtered_docs)} relevant documents for query")
            return filtered_docs
            
        except Exception as e:
            logger.error(f"Search error: {e}")
            return []
    
    def generate_answer(self, query: str, context_docs: List[Document]) -> Dict[str, Any]:
        """
        T·∫°o c√¢u tr·∫£ l·ªùi v·ªõi metadata chi ti·∫øt
        """
        if not context_docs:
            return {
                'answer': "ü§î T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y.",
                'confidence': 0,
                'sources_used': 0
            }
        
        # T·∫°o context t·ª´ documents
        context_parts = []
        for i, doc in enumerate(context_docs):
            source_name = doc.metadata.get('source', f'T√†i li·ªáu {i+1}')
            content = doc.page_content[:800]  # Limit content length
            context_parts.append(f"[Ngu·ªìn {i+1}: {source_name}]\n{content}")
        
        context = "\n\n".join(context_parts)
        
        # T·∫°o prompt v·ªõi instructions r√µ r√†ng
        prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, chuy√™n ph√¢n t√≠ch v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.

NGUY√äN T·∫ÆC TR·∫¢ L·ªúI:
1. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin c√≥ trong t√†i li·ªáu
2. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin, h√£y n√≥i r√µ
3. Tr√≠ch d·∫´n ngu·ªìn khi c·∫ßn thi·∫øt
4. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, r√µ r√†ng v√† chi ti·∫øt
5. N·∫øu c√≥ nhi·ªÅu quan ƒëi·ªÉm, h√£y t·ªïng h·ª£p

T√ÄI LI·ªÜU THAM KH·∫¢O:
{context}

C√ÇU H·ªéI: {query}

TR·∫¢ L·ªúI CHI TI·∫æT:
"""
        
        try:
            response = self.model.generate_content(prompt)
            answer = response.text
            
            # T√≠nh confidence score ƒë∆°n gi·∫£n
            confidence = min(len(context_docs) * 0.2, 1.0)
            
            return {
                'answer': answer,
                'confidence': confidence,
                'sources_used': len(context_docs)
            }
            
        except Exception as e:
            logger.error(f"Answer generation error: {e}")
            return {
                'answer': f"‚ùå L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}",
                'confidence': 0,
                'sources_used': 0
            }
    
    def chat(self, query: str) -> Dict[str, Any]:
        """
        X·ª≠ l√Ω c√¢u h·ªèi chat v·ªõi analytics
        """
        start_time = time.time()
        
        # T√¨m ki·∫øm documents li√™n quan
        similar_docs = self.search_similar_documents(query)
        
        # T·∫°o c√¢u tr·∫£ l·ªùi
        response_data = self.generate_answer(query, similar_docs)
        
        # Th√¥ng tin sources
        sources = []
        for doc in similar_docs:
            sources.append({
                'file_name': doc.metadata.get('source', 'Kh√¥ng r√µ'),
                'file_id': doc.metadata.get('file_id', ''),
                'web_view_link': doc.metadata.get('web_view_link', ''),
                'content_preview': doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                'content_length': doc.metadata.get('content_length', 0)
            })
        
        # Analytics
        processing_time = time.time() - start_time
        
        return {
            'answer': response_data['answer'],
            'sources': sources,
            'num_sources': len(sources),
            'confidence': response_data['confidence'],
            'processing_time': processing_time,
            'query_length': len(query)
        }

# Streamlit UI v·ªõi c·∫£i ti·∫øn
def main():
    st.set_page_config(
        page_title="RAG Chatbot - Google Drive & Gemini",
        page_icon="ü§ñ",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Custom CSS
    st.markdown("""
    <style>
    .main-header {
        text-align: center;
        padding: 1rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .source-card {
        background: #ffffff;
        padding: 1rem;
        border-left: 4px solid #667eea;
        margin: 0.5rem 0;
        border-radius: 5px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header
    st.markdown("""
    <div class="main-header">
        <h1>ü§ñ RAG Chatbot</h1>
        <p>Tr·ª£ l√Ω AI th√¥ng minh v·ªõi Google Drive & Gemini</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar c·∫•u h√¨nh
    with st.sidebar:
        st.header("‚öôÔ∏è C·∫•u h√¨nh")
        
        # API Key
        gemini_api_key = st.text_input(
            "üîë Gemini API Key",
            type="password",
            value=os.getenv('GEMINI_API_KEY', ''),
            help="Nh·∫≠p API key c·ªßa Gemini ho·∫∑c ƒë·∫∑t trong file .env"
        )
        
        # Folder ID
        folder_id = st.text_input(
            "üìÅ Google Drive Folder ID (t√πy ch·ªçn)",
            value=os.getenv('GOOGLE_DRIVE_FOLDER_ID', ''),
            help="ID c·ªßa folder c·∫ßn ƒë·ªçc file. ƒê·ªÉ tr·ªëng ƒë·ªÉ ƒë·ªçc t·∫•t c·∫£ file"
        )
        
        # Advanced settings
        with st.expander("üîß C√†i ƒë·∫∑t n√¢ng cao"):
            max_files = st.slider("S·ªë file t·ªëi ƒëa", 10, 200, 50)
            chunk_size = st.slider("K√≠ch th∆∞·ªõc chunk", 500, 2000, 1000)
            max_search_results = st.slider("S·ªë k·∫øt qu·∫£ t√¨m ki·∫øm", 3, 10, 5)
        
        # Lo·∫°i file
        file_types = st.multiselect(
            "üìÑ Lo·∫°i file c·∫ßn ƒë·ªçc",
            ['pdf', 'docx', 'txt', 'xlsx'],
            default=['pdf', 'docx', 'txt'],
            help="Ch·ªçn c√°c lo·∫°i file mu·ªën x·ª≠ l√Ω"
        )
        
        st.markdown("---")
        
        # Stats n·∫øu ƒë√£ load
        if 'chatbot' in st.session_state and st.session_state.get('documents_loaded'):
            st.markdown("### üìä Th·ªëng k√™")
            if 'file_stats' in st.session_state:
                stats = st.session_state.file_stats
                st.metric("üìÅ Files ƒë√£ t·∫£i", stats.get('total_files', 0))
                st.metric("üìù ƒêo·∫°n text", stats.get('total_chunks', 0))
                st.metric("üíæ Cache hits", len(st.session_state.chatbot.file_cache))
        
        st.markdown("---")
        
        # Quick actions
        if st.button("üóëÔ∏è X√≥a cache"):
            if os.path.exists(CACHE_DIR):
                import shutil
                shutil.rmtree(CACHE_DIR)
                os.makedirs(CACHE_DIR, exist_ok=True)
                st.success("Cache ƒë√£ ƒë∆∞·ª£c x√≥a")
                st.experimental_rerun()
        
        if st.button("üîÑ Reset ·ª©ng d·ª•ng"):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.experimental_rerun()
    
    # Initialize session state
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = None
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'documents_loaded' not in st.session_state:
        st.session_state.documents_loaded = False
    
    # Main content area
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # K·∫øt n·ªëi v√† t·∫£i t√†i li·ªáu
        if st.button("üöÄ K·∫øt n·ªëi & T·∫£i t√†i li·ªáu", type="primary"):
            if not gemini_api_key:
                st.error("‚ùå Vui l√≤ng nh·∫≠p Gemini API Key")
            else:
                with st.spinner("üîÑ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng..."):
                    try:
                        # Kh·ªüi t·∫°o chatbot
                        chatbot = GoogleDriveRAGChatbot(gemini_api_key)
                        
                        # C·∫≠p nh·∫≠t c·∫•u h√¨nh t·ª´ UI
                        chatbot.chunk_size = chunk_size
                        chatbot.max_search_results = max_search_results
                        
                        # K·∫øt n·ªëi Google Drive
                        if chatbot.authenticate_google_drive():
                            st.success("‚úÖ K·∫øt n·ªëi Google Drive th√†nh c√¥ng!")
                            
                            # L·∫•y danh s√°ch files
                            files = chatbot.get_files_from_drive(
                                folder_id if folder_id else None, 
                                file_types, 
                                max_files
                            )
                            
                            if files:
                                st.info(f"üìÅ T√¨m th·∫•y {len(files)} file(s) ph√π h·ª£p")
                                
                                # Hi·ªÉn th·ªã preview files
                                with st.expander("üëÄ Preview files s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω"):
                                    for file_info in files[:10]:  # Hi·ªÉn th·ªã 10 ƒë·∫ßu
                                        size_mb = int(file_info.get('size', 0)) / (1024*1024)
                                        st.write(f"üìÑ **{file_info['name']}** ({size_mb:.1f}MB)")
                                    if len(files) > 10:
                                        st.write(f"... v√† {len(files)-10} file kh√°c")
                                
                                # X·ª≠ l√Ω documents
                                documents = chatbot.process_documents(files)
                                
                                if documents:
                                    # T·∫°o vector store
                                    chatbot.create_vector_store(documents)
                                    
                                    # L∆∞u v√†o session state
                                    st.session_state.chatbot = chatbot
                                    st.session_state.documents_loaded = True
                                    
                                    # L∆∞u stats
                                    st.session_state.file_stats = {
                                        'total_files': len(files),
                                        'total_documents': len(documents),
                                        'total_chunks': sum(len(chatbot.text_splitter.split_text(doc.page_content)) for doc in documents),
                                        'total_size_mb': sum(int(f.get('size', 0)) for f in files) / (1024*1024)
                                    }
                                    
                                    st.success("üéâ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng ƒë·ªÉ chat!")
                                    
                                    # Hi·ªÉn th·ªã th·ªëng k√™
                                    stats = st.session_state.file_stats
                                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                                    with col_stat1:
                                        st.metric(" Files", stats['total_files'])
                                    with col_stat2:
                                        st.metric(" Documents", stats['total_documents'])
                                    with col_stat3:
                                        st.metric(" Chunks", stats['total_chunks'])
                                    
                                else:
                                    st.error("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu")
                            else:
                                st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file n√†o ph√π h·ª£p")
                        else:
                            st.error("‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi Google Drive")
                            
                    except Exception as e:
                        st.error(f"‚ùå L·ªói: {str(e)}")
                        logger.error(f"Setup error: {e}")
    
    with col2:
        # Tr·∫°ng th√°i h·ªá th·ªëng
        st.markdown("### üéØ Tr·∫°ng th√°i h·ªá th·ªëng")
        
        if st.session_state.documents_loaded:
            st.success("üü¢ ƒê√£ s·∫µn s√†ng")
            if 'file_stats' in st.session_state:
                stats = st.session_state.file_stats
                st.markdown(f"""
                <div class="metric-card">
                    <strong>üìä Th·ªëng k√™:</strong><br>
                    üìÅ Files: {stats.get('total_files', 0)}<br>
                    üìÑ Documents: {stats.get('total_documents', 0)}<br>
                    üìù Chunks: {stats.get('total_chunks', 0)}<br>
                    üíæ Size: {stats.get('total_size_mb', 0):.1f} MB
                </div>
                """, unsafe_allow_html=True)
        else:
            st.warning("üü° Ch∆∞a k·∫øt n·ªëi")
            st.info("üëÜ Nh·∫•n n√∫t 'K·∫øt n·ªëi & T·∫£i t√†i li·ªáu' ƒë·ªÉ b·∫Øt ƒë·∫ßu")
        
        # H∆∞·ªõng d·∫´n nhanh
        with st.expander("üí° M·∫πo s·ª≠ d·ª•ng"):
            st.markdown("""
            **C√¢u h·ªèi hi·ªáu qu·∫£:**
            - H·ªèi c·ª• th·ªÉ v·ªÅ n·ªôi dung t√†i li·ªáu
            - Y√™u c·∫ßu t√≥m t·∫Øt ho·∫∑c ph√¢n t√≠ch
            - T√¨m ki·∫øm th√¥ng tin chi ti·∫øt
            
            **V√≠ d·ª•:**
            - "T√≥m t·∫Øt n·ªôi dung ch√≠nh"
            - "T√¨m th√¥ng tin v·ªÅ..."
            - "So s√°nh gi·ªØa... v√†..."
            """)
    
    # Chat interface
    if st.session_state.documents_loaded and st.session_state.chatbot:
        st.markdown("---")
        st.header("üí¨ Tr√≤ chuy·ªán v·ªõi AI")
        
        # Chat container
        chat_container = st.container()
        
        with chat_container:
            # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
            for i, message in enumerate(st.session_state.messages):
                with st.chat_message(message["role"]):
                    st.write(message["content"])
                    
                    # Hi·ªÉn th·ªã metadata cho assistant
                    if message["role"] == "assistant" and "metadata" in message:
                        metadata = message["metadata"]
                        
                        # Confidence v√† timing
                        col_meta1, col_meta2, col_meta3 = st.columns(3)
                        with col_meta1:
                            confidence = metadata.get('confidence', 0)
                            st.caption(f"üéØ ƒê·ªô tin c·∫≠y: {confidence:.1%}")
                        with col_meta2:
                            processing_time = metadata.get('processing_time', 0)
                            st.caption(f"‚è±Ô∏è Th·ªùi gian: {processing_time:.1f}s")
                        with col_meta3:
                            num_sources = metadata.get('num_sources', 0)
                            st.caption(f"üìö Ngu·ªìn: {num_sources}")
                        
                        # Hi·ªÉn th·ªã sources
                        if metadata.get('sources'):
                            with st.expander(f"üìö Ngu·ªìn tham kh·∫£o ({len(metadata['sources'])} t√†i li·ªáu)", expanded=False):
                                for j, source in enumerate(metadata['sources'], 1):
                                    st.markdown(f"""
                                    <div class="source-card">
                                        <strong>{j}. {source['file_name']}</strong>
                                        {f'<br><a href="{source["web_view_link"]}" target="_blank">üîó Xem file</a>' if source.get('web_view_link') else ''}
                                        <br><em>{source['content_preview']}</em>
                                    </div>
                                    """, unsafe_allow_html=True)
        
        # Input chat v·ªõi suggestions
        st.markdown("### üí≠ ƒê·∫∑t c√¢u h·ªèi")
        
        # Quick suggestions
        if not st.session_state.messages:
            st.markdown("**üí° G·ª£i √Ω c√¢u h·ªèi:**")
            suggestion_cols = st.columns(3)
            
            suggestions = [
                "T√≥m t·∫Øt n·ªôi dung ch√≠nh",
                "Nh·ªØng ƒëi·ªÉm quan tr·ªçng nh·∫•t",
                "C√≥ th√¥ng tin g√¨ v·ªÅ..."
            ]
            
            for i, suggestion in enumerate(suggestions):
                with suggestion_cols[i]:
                    if st.button(suggestion, key=f"suggestion_{i}"):
                        # T·ª± ƒë·ªông ƒëi·ªÅn suggestion v√†o chat
                        st.session_state.auto_query = suggestion
        
        # Chat input
        query = st.chat_input(
            "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...",
            key="chat_input"
        )
        
        # X·ª≠ l√Ω auto query t·ª´ suggestions
        if 'auto_query' in st.session_state:
            query = st.session_state.auto_query
            del st.session_state.auto_query
        
        if query:
            # Validate query
            if len(query.strip()) < 3:
                st.warning(" C√¢u h·ªèi qu√° ng·∫Øn. Vui l√≤ng nh·∫≠p c√¢u h·ªèi chi ti·∫øt h∆°n.")
                st.stop()
            
            # Hi·ªÉn th·ªã c√¢u h·ªèi
            with st.chat_message("user"):
                st.write(query)
            
            # Th√™m v√†o l·ªãch s·ª≠
            st.session_state.messages.append({"role": "user", "content": query})
            
            # T·∫°o c√¢u tr·∫£ l·ªùi
            with st.chat_message("assistant"):
                with st.spinner("ü§î ƒêang suy nghƒ©..."):
                    try:
                        response = st.session_state.chatbot.chat(query)
                        
                        # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
                        st.write(response['answer'])
                        
                        # Hi·ªÉn th·ªã metadata
                        col_meta1, col_meta2, col_meta3 = st.columns(3)
                        with col_meta1:
                            st.caption(f"ƒê·ªô tin c·∫≠y: {response['confidence']:.1%}")
                        with col_meta2:
                            st.caption(f"Th·ªùi gian: {response['processing_time']:.1f}s")
                        with col_meta3:
                            st.caption(f"üìö Ngu·ªìn: {response['num_sources']}")
                        
                        # Hi·ªÉn th·ªã sources
                        if response['sources']:
                            with st.expander(f"üìö Ngu·ªìn tham kh·∫£o ({response['num_sources']} t√†i li·ªáu)", expanded=False):
                                for j, source in enumerate(response['sources'], 1):
                                    st.markdown(f"""
                                    <div class="source-card">
                                        <strong>{j}. {source['file_name']}</strong>
                                        {f'<br><a href="{source["web_view_link"]}" target="_blank">üîó Xem file</a>' if source.get('web_view_link') else ''}
                                        <br><em>{source['content_preview']}</em>
                                    </div>
                                    """, unsafe_allow_html=True)
                        
                        # ƒê√°nh gi√° ph·∫£n h·ªìi
                        st.markdown("---")
                        feedback_col1, feedback_col2, feedback_col3 = st.columns([1, 1, 3])
                        
                        with feedback_col1:
                            if st.button("üëç", key=f"thumbs_up_{len(st.session_state.messages)}"):
                                st.success("C·∫£m ∆°n ph·∫£n h·ªìi!")
                        
                        with feedback_col2:
                            if st.button("üëé", key=f"thumbs_down_{len(st.session_state.messages)}"):
                                st.info("Ch√∫ng t√¥i s·∫Ω c·∫£i thi·ªán!")
                        
                        with feedback_col3:
                            st.caption("Ph·∫£n h·ªìi gi√∫p c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng tr·∫£ l·ªùi")
                        
                    except Exception as e:
                        st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")
                        logger.error(f"Chat error: {e}")
                        response = {
                            'answer': "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n.",
                            'sources': [],
                            'num_sources': 0,
                            'confidence': 0,
                            'processing_time': 0
                        }
            
            # Th√™m v√†o l·ªãch s·ª≠ v·ªõi metadata
            st.session_state.messages.append({
                "role": "assistant",
                "content": response['answer'],
                "metadata": {
                    'sources': response['sources'],
                    'num_sources': response['num_sources'],
                    'confidence': response['confidence'],
                    'processing_time': response['processing_time']
                }
            })
    
    else:
        # H∆∞·ªõng d·∫´n khi ch∆∞a setup
        st.markdown("---")
        st.info("üöÄ **H√£y k·∫øt n·ªëi v·ªõi Google Drive ƒë·ªÉ b·∫Øt ƒë·∫ßu!**")
        
        # Checklist setup
        st.markdown("### ‚úÖ Checklist chu·∫©n b·ªã:")
        
        checklist_items = [
            ("üìÅ File credentials.json", os.path.exists(CREDENTIALS_FILE)),
            ("üîë Gemini API Key", bool(gemini_api_key)),
            ("üìÑ Ch·ªçn lo·∫°i file", bool(file_types)),
        ]
        
        for item, status in checklist_items:
            if status:
                st.success(f"‚úÖ {item}")
            else:
                st.error(f"‚ùå {item}")
        
        # Demo queries
        if all(status for _, status in checklist_items):
            st.success("üéâ B·∫°n ƒë√£ s·∫µn s√†ng! Nh·∫•n n√∫t 'K·∫øt n·ªëi & T·∫£i t√†i li·ªáu' ph√≠a tr√™n.")
        else:
            st.warning("‚ö†Ô∏è Vui l√≤ng ho√†n th√†nh checklist tr∆∞·ªõc khi ti·∫øp t·ª•c.")

if __name__ == "__main__":
    main()
    from langchain_google_genai import GoogleGenerativeAIEmbeddings